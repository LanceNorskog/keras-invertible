{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Invertible Preso #2- Invertible Layers (Advanced).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Invertible Deep Learning"
      ],
      "metadata": {
        "id": "4UcoMiWhNM5c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Advanced Invertible Layers\n",
        "In this notebook we will demonstrate techniques for inverting a few other common layers in deep learning."
      ],
      "metadata": {
        "id": "75MOayr2NZ4f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Signed sin and arcsin\n",
        "The \"signed sin\" function extends sin to negatively valued numbers in a mirrored fashion, since sin is only defined for positive values.\n",
        "\n",
        "mathjax for signed sin"
      ],
      "metadata": {
        "id": "SH294QyoNqA3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_wQGOrKSNSVM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yApcOUhcNGlk"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = np.array([-4, -3, -2, -1, 0, 1, 2, 3, 4])\n",
        "signed_sin_data = data * np.sin(np.abs(data)) * np.sign(data)\n",
        "\n",
        "print(signed_sin_data)\n",
        "\n",
        "inverted = np.arcsin((1/data) * signed_sin_data) * np.sign(signed_sin_data)\n",
        "print(inverted)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZxxLM17OG71",
        "outputId": "b424c82c-382b-4858-c3e1-f2d0c588914f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-3.02720998  0.42336002  1.81859485  0.84147098  0.          0.84147098\n",
            "  1.81859485  0.42336002 -3.02720998]\n",
            "[-0.85840735 -0.14159265 -1.14159265 -1.                 nan  1.\n",
            "  1.14159265  0.14159265  0.85840735]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: RuntimeWarning: divide by zero encountered in true_divide\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: RuntimeWarning: invalid value encountered in multiply\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data = np.array([1, 2, 3, 4])\n",
        "forward = data * np.tanh(data)\n",
        "\n",
        "print(forward)\n",
        "\n",
        "inverted = np.arctanh(forward/data) \n",
        "print(inverted)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5tIz9AuObSE",
        "outputId": "04d0e7b6-1d13-4265-b75c-90bd6db01adb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.76159416 1.92805516 2.98516426 3.9973172 ]\n",
            "[1. 2. 3. 4.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data = np.array([-1, -0.5, 0, 0.5, 1, 2, 3, 4])\n",
        "forward = np.abs(data) * np.tanh(data)\n",
        "\n",
        "print(forward)\n",
        "\n",
        "inverted = np.arctanh(np.abs(forward)/np.abs(data))*np.sign(data) \n",
        "print(inverted)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXs2vIXGQFeP",
        "outputId": "b7b9fa42-2691-45f8-a140-580a7ecc50a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.76159416 -0.23105858  0.          0.23105858  0.76159416  1.92805516\n",
            "  2.98516426  3.9973172 ]\n",
            "[-1.  -0.5  nan  0.5  1.   2.   3.   4. ]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: RuntimeWarning: invalid value encountered in true_divide\n",
            "  import sys\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add a filter for 0, and recode in tensorflow, and you've got a pair of invertible activation functions. Tanh was popular in earlier research, but its only current usage is in LSTM/GRU, and I am not going to invert *those*."
      ],
      "metadata": {
        "id": "UeKJYvzSSKw5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parametric ReLU (Monotonic variant)\n",
        "\n",
        "A more powerful version of LeakyReLU is Parametric ReLU, which has a learnable vector of alpha values for all hidden neurons. This can be used to modulate feature maps in Convolutional networks. We will present a simplified variant and its inverse.\n",
        "\n",
        "The problem with Parametric ReLU as published (in the same paper that introduced He kernel initialization!) is that the alpha values are allowed to be negative. This means that the function is not monotonic. It is easy to alter Parametric ReLU to be monotonic, with the simple application of abs(). Here is the Keras source for Parametric ReLU:\n",
        "\n",
        "```\n",
        "    def call(self, inputs):\n",
        "        pos = tf.keras.backend.relu(inputs)\n",
        "        neg = -self.alpha * tf.keras.backend.relu(-inputs)\n",
        "        return pos + neg\n",
        "```\n",
        "\n",
        "This change forces monotonicity:\n",
        "```\n",
        "    def call(self, inputs):\n",
        "        pos = tf.keras.backend.relu(inputs)\n",
        "        neg = -tf.math.abs(self.alpha) * tf.keras.backend.relu(-inputs)\n",
        "        return pos + neg\n",
        "```\n",
        "\n",
        "To invert this layer, we need to capture the layer object in another layer and refer to it, inverting the alpha values:\n",
        "```\n",
        "    def call(self, inputs):\n",
        "        pos = tf.keras.backend.relu(inputs)\n",
        "        neg = -tf.math.abs(1/tf.math.abs(self.other_layer.alpha)) * tf.keras.backend.relu(-inputs)\n",
        "        return pos + neg\n",
        "```\n",
        "\n",
        "Taking the abs() value of the alpha vector violates the probability theory behind weight initialization (Glorot, He, etc) but it seems to work fine."
      ],
      "metadata": {
        "id": "PifqWERzSmjm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class MonotonicPReLU(tf.keras.layers.Layer):\n",
        "    \"\"\"Monotonic Parametric Rectified Linear Unit.\n",
        "    It follows:\n",
        "    ```\n",
        "      f(x) = alpha * x for x < 0\n",
        "      f(x) = x for x >= 0\n",
        "    ```\n",
        "    where `alpha` is a learned array with the same shape as x.\n",
        "    To achieve monotonicity, force learned alpha params above 0 via abs().\n",
        "    Input shape:\n",
        "      Arbitrary. Use the keyword argument `input_shape`\n",
        "      (tuple of integers, does not include the samples axis)\n",
        "      when using this layer as the first layer in a model.\n",
        "    Output shape:\n",
        "      Same shape as the input.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        param_shape = list(input_shape[1:])\n",
        "        self.alpha = self.add_weight(\n",
        "            shape=param_shape,\n",
        "            name=\"alpha\",\n",
        "            initializer='glorot_uniform'\n",
        "        )\n",
        "        self.input_spec = tf.keras.layers.InputSpec(ndim=len(input_shape), axes={})\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs):\n",
        "        pos = tf.keras.backend.relu(inputs)\n",
        "        neg = -tf.math.abs(self.alpha) * tf.keras.backend.relu(-inputs)\n",
        "        return pos + neg\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape\n",
        "\n",
        "    \n",
        "class InvertedMonotonicPReLU(tf.keras.layers.Layer):\n",
        "    \"\"\"Inverted match of Monotonic Parametric Rectified Linear Unit.\n",
        "        Supplies inverted version of given layer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        other_layer,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.other_layer = other_layer\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        param_shape = list(input_shape[1:])\n",
        "        self.input_spec = tf.keras.layers.InputSpec(ndim=len(input_shape), axes={})\n",
        "        self.params = []\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs):\n",
        "        pos = tf.keras.backend.relu(inputs)\n",
        "        neg = -(1/tf.math.abs(self.other_layer.alpha)) * tf.keras.backend.relu(-inputs)\n",
        "        return pos + neg\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape"
      ],
      "metadata": {
        "id": "M9bqLCWWS7gZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will reuse the utility functions from Preso #1 and demonstrate that MonotonicPReLU and InvertedMonotonicPReLU form a mirrored pair and can be used in deep learning."
      ],
      "metadata": {
        "id": "SaZ5N5ZxW1kH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(shape, layer):\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(tf.keras.Input(shape=shape))\n",
        "    model.add(layer)\n",
        "    model.compile(optimizer='sgd', loss='mse')\n",
        "    return model\n",
        "\n",
        "def delta_check(data1, data2, epsilon=1e-2):\n",
        "    delta = np.abs(data1 - data2)\n",
        "    return np.max(delta) < epsilon"
      ],
      "metadata": {
        "id": "OpNWOt0-W1S5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "forward_layer = MonotonicPReLU()\n",
        "forward_prelu = create_model((3), forward_layer)\n",
        "inverted_layer = InvertedMonotonicPReLU(forward_layer)\n",
        "inverted_prelu = create_model((3), inverted_layer)\n",
        "\n",
        "data = np.array([[-1, 0, 1]])\n",
        "forward = forward_prelu.predict(data)\n",
        "print(forward)\n",
        "inverted = inverted_prelu.predict(forward)\n",
        "print(inverted)\n",
        "print(delta_check(data, inverted))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iturSETnYMft",
        "outputId": "a8f5ff30-3d6f-4d28-ba70-c3dcc353c5e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.1929388  0.         1.       ]]\n",
            "[[-1.  0.  1.]]\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This demonstrates that the MonotonicPRelu/InvertedMonotonicPReLU pair do indeed create a mirrored pair of layers which invert each other's actions."
      ],
      "metadata": {
        "id": "RFNfYue8ZNnq"
      }
    }
  ]
}