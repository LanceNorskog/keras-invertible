{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Palindromic Deep Learning for MNIST Digits 3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Start with a simple multi-stage autoencoder.\n",
        "\n",
        "Use pRELU since it should be invertible\n",
        "\n",
        "https://blog.keras.io/building-autoencoders-in-keras.html"
      ],
      "metadata": {
        "id": "cKDtwP6aK3AX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "O6-EZfoSkDis"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.ops import nn\n",
        "import keras\n",
        "from keras import layers\n",
        "from keras.datasets import mnist\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "(x_train, _), (x_test, _) = mnist.load_data()\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
        "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCuLZ05oklTv",
        "outputId": "597bd94b-9d5b-4cdc-82a4-14e384799710"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 784)\n",
            "(10000, 784)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model():\n",
        "    input_img = keras.Input(shape=(784,))\n",
        "    encoded = layers.Dense(128)(input_img)\n",
        "    encoded = layers.PReLU()(encoded)\n",
        "    encoded = layers.Dense(64)(encoded)\n",
        "    encoded = layers.PReLU()(encoded)\n",
        "    encoded = layers.Dense(32)(encoded)\n",
        "    encoded = layers.PReLU()(encoded)\n",
        "\n",
        "    decoded = layers.Dense(64)(encoded)\n",
        "    encoded = layers.PReLU()(encoded)\n",
        "    decoded = layers.Dense(128)(decoded)\n",
        "    encoded = layers.PReLU()(encoded)\n",
        "    decoded = layers.Dense(784, activation='sigmoid')(decoded)\n",
        "\n",
        "    autoencoder = keras.Model(input_img, decoded)\n",
        "    autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "    autoencoder.summary()\n",
        "    return autoencoder\n"
      ],
      "metadata": {
        "id": "_7EusGw9l35D"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_model()\n",
        "model.fit(x_train, x_train,\n",
        "                epochs=5,\n",
        "                batch_size=256,\n",
        "                shuffle=True)\n",
        "prelu = []\n",
        "for layer in model.layers:\n",
        "    if 'p_re_lu' in layer.name:\n",
        "        prelu.append(layer)\n",
        "print(prelu)"
      ],
      "metadata": {
        "id": "T8pVaeGCp1cj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69144341-f620-4de8-9029-1bec36eb439a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 784)]             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               100480    \n",
            "                                                                 \n",
            " p_re_lu (PReLU)             (None, 128)               128       \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " p_re_lu_1 (PReLU)           (None, 64)                64        \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 32)                2080      \n",
            "                                                                 \n",
            " p_re_lu_2 (PReLU)           (None, 32)                32        \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 64)                2112      \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 128)               8320      \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 784)               101136    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 222,608\n",
            "Trainable params: 222,608\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "235/235 [==============================] - 9s 31ms/step - loss: 0.2305\n",
            "Epoch 2/5\n",
            "235/235 [==============================] - 8s 32ms/step - loss: 0.1438\n",
            "Epoch 3/5\n",
            "235/235 [==============================] - 8s 32ms/step - loss: 0.1261\n",
            "Epoch 4/5\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.1180\n",
            "Epoch 5/5\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 0.1130\n",
            "[<keras.layers.advanced_activations.PReLU object at 0x7f656fc27790>, <keras.layers.advanced_activations.PReLU object at 0x7f656bee9c10>, <keras.layers.advanced_activations.PReLU object at 0x7f656bef5950>]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for p in prelu:\n",
        "    print(p.trainable_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUSQ5d7mwlOW",
        "outputId": "8170a636-697d-4426-c57a-0634a9454254"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[<tf.Variable 'p_re_lu/alpha:0' shape=(128,) dtype=float32, numpy=\n",
            "array([ 9.71000548e-03,  7.32508302e-02,  2.85959523e-02, -3.82519588e-02,\n",
            "        2.90668815e-01,  1.69363156e-01,  2.45097190e-01,  6.92224801e-02,\n",
            "       -5.32902889e-02,  9.22408514e-03,  1.10214159e-01,  4.06620741e-01,\n",
            "        1.24493331e-01,  3.46499383e-02,  3.02098729e-02,  4.21229675e-02,\n",
            "        2.98868477e-01,  2.22231708e-02,  1.12229533e-01,  2.89825916e-01,\n",
            "        3.25130165e-01, -5.59885390e-02,  1.35369763e-01,  1.30249247e-01,\n",
            "       -1.26103759e-02,  3.42662275e-01, -2.90157795e-02, -9.65612009e-02,\n",
            "        1.44173875e-01,  4.54036929e-02,  1.65743247e-01,  6.32910952e-02,\n",
            "        2.11370625e-02,  2.11678296e-01,  1.25232348e-02,  1.32398292e-01,\n",
            "        6.15807056e-01,  1.80929542e-01,  2.34870955e-01, -4.58888300e-02,\n",
            "        1.54503629e-01, -8.20528995e-03,  1.62208937e-02, -6.45974725e-02,\n",
            "        6.33314773e-02,  2.06263199e-01, -2.18371954e-02,  2.42061242e-01,\n",
            "        2.03945354e-01, -1.22664450e-02,  2.66596287e-01,  9.93652418e-02,\n",
            "        9.88856107e-02,  1.53210089e-01, -5.71680777e-02,  4.94368747e-02,\n",
            "        2.05393299e-01,  1.78902131e-02,  3.17077190e-01,  2.55800843e-01,\n",
            "       -1.52864475e-02,  2.20335335e-01,  3.14946733e-02,  1.34599274e-02,\n",
            "       -7.78835267e-02,  1.54341787e-01,  3.80928516e-01,  2.45806485e-01,\n",
            "        2.84465075e-01,  1.50934188e-02, -1.43878348e-02,  2.34600514e-01,\n",
            "        7.51729235e-02,  2.13643372e-01,  7.11393505e-02, -1.23815127e-01,\n",
            "       -4.32926882e-03,  2.75603607e-02,  1.49285048e-01,  2.14994848e-01,\n",
            "        1.39039949e-01,  7.01290295e-02, -5.74928932e-02,  4.46177535e-02,\n",
            "        2.41342321e-01,  4.62799929e-02,  4.89503406e-02,  1.66762754e-01,\n",
            "        2.41732284e-01, -4.54005972e-03,  1.01997271e-01,  4.13742103e-02,\n",
            "       -6.80320710e-02,  8.82252492e-03,  5.08445017e-02,  1.87278334e-02,\n",
            "       -7.42452666e-02,  9.60477963e-02,  5.16416971e-03, -9.16333124e-03,\n",
            "        1.33603856e-01,  2.10742950e-01,  4.38435040e-02,  8.21922161e-03,\n",
            "       -9.87442472e-05,  1.93024144e-01, -8.07401836e-02,  1.87361822e-01,\n",
            "        2.00332746e-01, -2.24031415e-02,  1.56527355e-01, -4.23848443e-02,\n",
            "        1.59523249e-01,  6.84592575e-02,  4.38913293e-02,  3.76401981e-03,\n",
            "       -5.46306819e-02,  8.33855271e-02,  5.81071414e-02,  5.61144501e-02,\n",
            "        4.39076692e-01, -6.57723993e-02,  3.26611429e-01,  1.84144899e-01,\n",
            "       -1.78160235e-01,  1.77290216e-02,  3.52010422e-04,  1.81584075e-01],\n",
            "      dtype=float32)>]\n",
            "[<tf.Variable 'p_re_lu_1/alpha:0' shape=(64,) dtype=float32, numpy=\n",
            "array([-0.08459727,  0.02970668,  0.18978347,  0.24316248,  0.02958869,\n",
            "        0.27815098, -0.16556853,  0.10771832,  0.08422387,  0.06842755,\n",
            "        0.00283065,  0.07932265, -0.03027955,  0.45853427,  0.04370722,\n",
            "        0.03896458,  0.06004128,  0.19605875,  0.17862965,  0.13282186,\n",
            "        0.08710887,  0.12032916,  0.07790282,  0.31616917,  0.16128041,\n",
            "        0.09134354,  0.16374831, -0.05199543,  0.03486546,  0.02624284,\n",
            "        0.07970461,  0.14253949,  0.07366389,  0.07723868,  0.05209094,\n",
            "        0.24717496,  0.05653078,  0.2651357 ,  0.03388922,  0.03959306,\n",
            "       -0.01941862,  0.09318934,  0.03697772,  0.05655083,  0.22435327,\n",
            "        0.09984128,  0.10614087,  0.10313675,  0.72122836,  0.08839626,\n",
            "        0.03612828,  0.11355543,  0.07348163,  0.0060852 ,  0.5491415 ,\n",
            "        0.26610938, -0.00114821,  0.17477882,  0.24340339,  0.16727704,\n",
            "        0.19019268,  0.6014464 ,  0.11236307,  0.02997275], dtype=float32)>]\n",
            "[<tf.Variable 'p_re_lu_2/alpha:0' shape=(32,) dtype=float32, numpy=\n",
            "array([ 0.0839026 ,  0.10446625,  0.4065582 ,  0.3796923 ,  0.32916716,\n",
            "       -0.04306151,  0.5577704 ,  0.06742236, -0.03867987,  0.39177316,\n",
            "        0.66426235, -0.00613852,  0.32710952,  0.3913605 , -0.02634487,\n",
            "        0.2022771 ,  0.31886134,  0.01764531,  0.        ,  0.25294468,\n",
            "        0.17672023,  0.30525354,  0.33727393, -0.04026616,  0.00372353,\n",
            "        0.30101228,  0.3656675 ,  0.62044257,  0.27083576,  0.1976221 ,\n",
            "        0.02071282,  0.11437317], dtype=float32)>]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TiedDense(layers.Layer):\n",
        "    def __init__(self, master_layer):\n",
        "        super(TiedDense, self).__init__()\n",
        "        self.master_layer = master_layer\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # do not train weights or bias from master_layer, they are read-only\n",
        "        self.params = []\n",
        "        \n",
        "    def call(self, inputs):  # Defines the computation from inputs to outputs\n",
        "        W = self.master_layer._trainable_weights[0]\n",
        "        b = self.master_layer._trainable_weights[1]\n",
        "        w = tf.transpose(W)\n",
        "        return tf.matmul(inputs - b, w)\n",
        "    "
      ],
      "metadata": {
        "id": "C9LallIk5HHD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class TiedPReLU(layers.Layer):\n",
        "    def __init__(self, master_layer):\n",
        "        super(TiedPReLU, self).__init__()\n",
        "        self.master_layer = master_layer\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # do not train weights or bias from master_layer, they are read-only\n",
        "        self.params = []\n",
        "\n",
        "    def call(self, inputs):\n",
        "        alpha = 1/(self.master_layer.alpha + 0.00001)\n",
        "        pos = keras.backend.relu(inputs)\n",
        "        neg = -alpha * keras.backend.relu(-inputs)\n",
        "        return pos + neg\n"
      ],
      "metadata": {
        "id": "IZ8KUvUqy2rj"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kernel_init='he_normal'\n",
        "paper_init=keras.initializers.Constant(0.25)\n",
        "def create_palindromic_model():\n",
        "    input_img = keras.Input(shape=(784,))\n",
        "    a = layers.Dense(128, kernel_initializer=kernel_init)\n",
        "    encoded = a(input_img)\n",
        "    a_p = layers.PReLU(alpha_initializer=paper_init)\n",
        "    encoded = a_p(encoded)\n",
        "    # encoded = layers.Dropout(0.2)(encoded)\n",
        "    b = layers.Dense(64, kernel_initializer=kernel_init)\n",
        "    encoded = b(encoded)\n",
        "    b_p = layers.PReLU(alpha_initializer=paper_init)\n",
        "    encoded = b_p(encoded)\n",
        "    c = layers.Dense(32, kernel_initializer=kernel_init)\n",
        "    encoded = c(encoded)\n",
        "    c_p = layers.PReLU(alpha_initializer=paper_init)\n",
        "    encoded = c_p(encoded)\n",
        "\n",
        "    embedding = encoded\n",
        "\n",
        "    decoded = TiedPReLU(c_p)(embedding)\n",
        "    decoded = TiedDense(c)(decoded)\n",
        "    decoded = TiedPReLU(b_p)(decoded)\n",
        "    decoded = TiedDense(b)(decoded)\n",
        "    decoded = TiedPReLU(a_p)(decoded)\n",
        "    decoded = TiedDense(a)(decoded)\n",
        "    decoded = layers.Activation(activation='sigmoid')(decoded)\n",
        "\n",
        "    autoencoder = keras.Model(input_img, decoded)\n",
        "    autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "    autoencoder.summary()\n",
        "    return autoencoder\n"
      ],
      "metadata": {
        "id": "ZVpLJHQEpDwO"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 300\n",
        "autoencoder = create_palindromic_model()\n",
        "\n",
        "autoencoder.fit(x_train, x_train,\n",
        "                epochs=num_epochs,\n",
        "                batch_size=256,\n",
        "                shuffle=True,\n",
        "                validation_data=(x_test, x_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8AHMi-ttlDJ",
        "outputId": "fbdb6ab7-d047-41c1-82d7-9acf37b5e447"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_8 (InputLayer)        [(None, 784)]             0         \n",
            "                                                                 \n",
            " dense_24 (Dense)            (None, 128)               100480    \n",
            "                                                                 \n",
            " p_re_lu_23 (PReLU)          (None, 128)               128       \n",
            "                                                                 \n",
            " dense_25 (Dense)            (None, 64)                8256      \n",
            "                                                                 \n",
            " p_re_lu_24 (PReLU)          (None, 64)                64        \n",
            "                                                                 \n",
            " dense_26 (Dense)            (None, 32)                2080      \n",
            "                                                                 \n",
            " p_re_lu_25 (PReLU)          (None, 32)                32        \n",
            "                                                                 \n",
            " tied_p_re_lu_18 (TiedPReLU)  (None, 32)               32        \n",
            "                                                                 \n",
            " tied_dense_18 (TiedDense)   (None, 64)                2080      \n",
            "                                                                 \n",
            " tied_p_re_lu_19 (TiedPReLU)  (None, 64)               64        \n",
            "                                                                 \n",
            " tied_dense_19 (TiedDense)   (None, 128)               8256      \n",
            "                                                                 \n",
            " tied_p_re_lu_20 (TiedPReLU)  (None, 128)              128       \n",
            "                                                                 \n",
            " tied_dense_20 (TiedDense)   (None, 784)               100480    \n",
            "                                                                 \n",
            " activation_5 (Activation)   (None, 784)               0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 111,040\n",
            "Trainable params: 111,040\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/300\n",
            "235/235 [==============================] - 6s 21ms/step - loss: 0.2512 - val_loss: 0.1561\n",
            "Epoch 2/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.1365 - val_loss: 0.1211\n",
            "Epoch 3/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.1163 - val_loss: 0.1099\n",
            "Epoch 4/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.1080 - val_loss: 0.1034\n",
            "Epoch 5/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.1030 - val_loss: 0.0997\n",
            "Epoch 6/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0999 - val_loss: 0.0974\n",
            "Epoch 7/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0980 - val_loss: 0.0967\n",
            "Epoch 8/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0965 - val_loss: 0.0944\n",
            "Epoch 9/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0954 - val_loss: 0.0935\n",
            "Epoch 10/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0944 - val_loss: 0.0927\n",
            "Epoch 11/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0936 - val_loss: 0.0923\n",
            "Epoch 12/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0927 - val_loss: 0.0912\n",
            "Epoch 13/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0920 - val_loss: 0.0905\n",
            "Epoch 14/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0916 - val_loss: 0.0902\n",
            "Epoch 15/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0910 - val_loss: 0.0900\n",
            "Epoch 16/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0906 - val_loss: 0.0897\n",
            "Epoch 17/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0902 - val_loss: 0.0891\n",
            "Epoch 18/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0899 - val_loss: 0.0887\n",
            "Epoch 19/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0896 - val_loss: 0.0883\n",
            "Epoch 20/300\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.0892 - val_loss: 0.0879\n",
            "Epoch 21/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0890 - val_loss: 0.0878\n",
            "Epoch 22/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0886 - val_loss: 0.0874\n",
            "Epoch 23/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0884 - val_loss: 0.0872\n",
            "Epoch 24/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0881 - val_loss: 0.0870\n",
            "Epoch 25/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0879 - val_loss: 0.0867\n",
            "Epoch 26/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0876 - val_loss: 0.0866\n",
            "Epoch 27/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0874 - val_loss: 0.0862\n",
            "Epoch 28/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0872 - val_loss: 0.0860\n",
            "Epoch 29/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.1007 - val_loss: 0.1122\n",
            "Epoch 30/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0957 - val_loss: 0.0903\n",
            "Epoch 31/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0903 - val_loss: 0.0884\n",
            "Epoch 32/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0891 - val_loss: 0.0876\n",
            "Epoch 33/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0884 - val_loss: 0.0870\n",
            "Epoch 34/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0878 - val_loss: 0.0866\n",
            "Epoch 35/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0874 - val_loss: 0.0862\n",
            "Epoch 36/300\n",
            "235/235 [==============================] - 6s 24ms/step - loss: 0.0871 - val_loss: 0.0861\n",
            "Epoch 37/300\n",
            "235/235 [==============================] - 6s 25ms/step - loss: 0.0869 - val_loss: 0.0870\n",
            "Epoch 38/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0868 - val_loss: 0.0861\n",
            "Epoch 39/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0866 - val_loss: 0.0857\n",
            "Epoch 40/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0866 - val_loss: 0.0856\n",
            "Epoch 41/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0862 - val_loss: 0.0853\n",
            "Epoch 42/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0861 - val_loss: 0.0852\n",
            "Epoch 43/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0860 - val_loss: 0.0849\n",
            "Epoch 44/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0858 - val_loss: 0.0848\n",
            "Epoch 45/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0857 - val_loss: 0.0846\n",
            "Epoch 46/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0856 - val_loss: 0.0848\n",
            "Epoch 47/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0854 - val_loss: 0.0843\n",
            "Epoch 48/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0855 - val_loss: 0.0844\n",
            "Epoch 49/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0853 - val_loss: 0.0843\n",
            "Epoch 50/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0851 - val_loss: 0.0843\n",
            "Epoch 51/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0851 - val_loss: 0.0841\n",
            "Epoch 52/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0849 - val_loss: 0.0840\n",
            "Epoch 53/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0848 - val_loss: 0.0839\n",
            "Epoch 54/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0847 - val_loss: 0.0839\n",
            "Epoch 55/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0845 - val_loss: 0.0837\n",
            "Epoch 56/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0846 - val_loss: 0.0837\n",
            "Epoch 57/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0844 - val_loss: 0.0837\n",
            "Epoch 58/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0843 - val_loss: 0.0833\n",
            "Epoch 59/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0843 - val_loss: 0.0834\n",
            "Epoch 60/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0841 - val_loss: 0.0836\n",
            "Epoch 61/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0845 - val_loss: 0.0836\n",
            "Epoch 62/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0842 - val_loss: 0.0832\n",
            "Epoch 63/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0838 - val_loss: 0.0829\n",
            "Epoch 64/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0837 - val_loss: 0.0829\n",
            "Epoch 65/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0836 - val_loss: 0.0828\n",
            "Epoch 66/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0835 - val_loss: 0.0828\n",
            "Epoch 67/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0835 - val_loss: 0.0826\n",
            "Epoch 68/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0834 - val_loss: 0.0827\n",
            "Epoch 69/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0845 - val_loss: 0.0828\n",
            "Epoch 70/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0834 - val_loss: 0.0829\n",
            "Epoch 71/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0831 - val_loss: 0.0825\n",
            "Epoch 72/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0831 - val_loss: 0.0826\n",
            "Epoch 73/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0830 - val_loss: 0.0826\n",
            "Epoch 74/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0831 - val_loss: 0.0821\n",
            "Epoch 75/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0829 - val_loss: 0.0822\n",
            "Epoch 76/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0828 - val_loss: 0.0820\n",
            "Epoch 77/300\n",
            "235/235 [==============================] - 6s 25ms/step - loss: 0.0827 - val_loss: 0.0819\n",
            "Epoch 78/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0826 - val_loss: 0.0820\n",
            "Epoch 79/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0826 - val_loss: 0.0818\n",
            "Epoch 80/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0825 - val_loss: 0.0820\n",
            "Epoch 81/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0824 - val_loss: 0.0818\n",
            "Epoch 82/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0825 - val_loss: 0.0819\n",
            "Epoch 83/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0823 - val_loss: 0.0817\n",
            "Epoch 84/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0823 - val_loss: 0.0815\n",
            "Epoch 85/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0835 - val_loss: 0.0819\n",
            "Epoch 86/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0822 - val_loss: 0.0814\n",
            "Epoch 87/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0821 - val_loss: 0.0814\n",
            "Epoch 88/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0820 - val_loss: 0.0817\n",
            "Epoch 89/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0820 - val_loss: 0.0813\n",
            "Epoch 90/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0819 - val_loss: 0.0814\n",
            "Epoch 91/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0819 - val_loss: 0.0814\n",
            "Epoch 92/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0818 - val_loss: 0.0813\n",
            "Epoch 93/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0819 - val_loss: 0.0813\n",
            "Epoch 94/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0818 - val_loss: 0.0812\n",
            "Epoch 95/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0817 - val_loss: 0.0811\n",
            "Epoch 96/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0816 - val_loss: 0.0810\n",
            "Epoch 97/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0817 - val_loss: 0.0811\n",
            "Epoch 98/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0816 - val_loss: 0.0808\n",
            "Epoch 99/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0815 - val_loss: 0.0807\n",
            "Epoch 100/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0815 - val_loss: 0.0810\n",
            "Epoch 101/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0814 - val_loss: 0.0811\n",
            "Epoch 102/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0814 - val_loss: 0.0808\n",
            "Epoch 103/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0815 - val_loss: 0.0812\n",
            "Epoch 104/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0813 - val_loss: 0.0807\n",
            "Epoch 105/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0813 - val_loss: 0.0807\n",
            "Epoch 106/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0812 - val_loss: 0.0808\n",
            "Epoch 107/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0812 - val_loss: 0.0807\n",
            "Epoch 108/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0812 - val_loss: 0.0807\n",
            "Epoch 109/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0811 - val_loss: 0.0804\n",
            "Epoch 110/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0812 - val_loss: 0.0808\n",
            "Epoch 111/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0811 - val_loss: 0.0806\n",
            "Epoch 112/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0811 - val_loss: 0.0804\n",
            "Epoch 113/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0811 - val_loss: 0.0806\n",
            "Epoch 114/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0809 - val_loss: 0.0804\n",
            "Epoch 115/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0810 - val_loss: 0.0808\n",
            "Epoch 116/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0810 - val_loss: 0.0804\n",
            "Epoch 117/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0809 - val_loss: 0.0803\n",
            "Epoch 118/300\n",
            "235/235 [==============================] - 6s 25ms/step - loss: 0.0808 - val_loss: 0.0803\n",
            "Epoch 119/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0808 - val_loss: 0.0803\n",
            "Epoch 120/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0808 - val_loss: 0.0804\n",
            "Epoch 121/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0808 - val_loss: 0.0803\n",
            "Epoch 122/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0807 - val_loss: 0.0805\n",
            "Epoch 123/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0807 - val_loss: 0.0803\n",
            "Epoch 124/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0807 - val_loss: 0.0802\n",
            "Epoch 125/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0807 - val_loss: 0.0804\n",
            "Epoch 126/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0806 - val_loss: 0.0801\n",
            "Epoch 127/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0814 - val_loss: 0.0811\n",
            "Epoch 128/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0807 - val_loss: 0.0800\n",
            "Epoch 129/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0805 - val_loss: 0.0800\n",
            "Epoch 130/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0805 - val_loss: 0.0801\n",
            "Epoch 131/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0805 - val_loss: 0.0799\n",
            "Epoch 132/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0805 - val_loss: 0.0799\n",
            "Epoch 133/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0804 - val_loss: 0.0800\n",
            "Epoch 134/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0804 - val_loss: 0.0798\n",
            "Epoch 135/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0804 - val_loss: 0.0799\n",
            "Epoch 136/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0804 - val_loss: 0.0799\n",
            "Epoch 137/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0803 - val_loss: 0.0799\n",
            "Epoch 138/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0803 - val_loss: 0.0799\n",
            "Epoch 139/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0803 - val_loss: 0.0799\n",
            "Epoch 140/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0803 - val_loss: 0.0801\n",
            "Epoch 141/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0803 - val_loss: 0.0796\n",
            "Epoch 142/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0802 - val_loss: 0.0797\n",
            "Epoch 143/300\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.0802 - val_loss: 0.0797\n",
            "Epoch 144/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0801 - val_loss: 0.0797\n",
            "Epoch 145/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0802 - val_loss: 0.0799\n",
            "Epoch 146/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0801 - val_loss: 0.0796\n",
            "Epoch 147/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0801 - val_loss: 0.0798\n",
            "Epoch 148/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0801 - val_loss: 0.0796\n",
            "Epoch 149/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0801 - val_loss: 0.0795\n",
            "Epoch 150/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0800 - val_loss: 0.0796\n",
            "Epoch 151/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0801 - val_loss: 0.0795\n",
            "Epoch 152/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0800 - val_loss: 0.0794\n",
            "Epoch 153/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0799 - val_loss: 0.0794\n",
            "Epoch 154/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0799 - val_loss: 0.0795\n",
            "Epoch 155/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0800 - val_loss: 0.0797\n",
            "Epoch 156/300\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0799 - val_loss: 0.0794\n",
            "Epoch 157/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0799 - val_loss: 0.0794\n",
            "Epoch 158/300\n",
            "235/235 [==============================] - 6s 25ms/step - loss: 0.0798 - val_loss: 0.0795\n",
            "Epoch 159/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0799 - val_loss: 0.0795\n",
            "Epoch 160/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0798 - val_loss: 0.0793\n",
            "Epoch 161/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0798 - val_loss: 0.0794\n",
            "Epoch 162/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0798 - val_loss: 0.0793\n",
            "Epoch 163/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0823 - val_loss: 0.1003\n",
            "Epoch 164/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0839 - val_loss: 0.0806\n",
            "Epoch 165/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0808 - val_loss: 0.0799\n",
            "Epoch 166/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0803 - val_loss: 0.0797\n",
            "Epoch 167/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0800 - val_loss: 0.0795\n",
            "Epoch 168/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0799 - val_loss: 0.0798\n",
            "Epoch 169/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0799 - val_loss: 0.0793\n",
            "Epoch 170/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0798 - val_loss: 0.0793\n",
            "Epoch 171/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0798 - val_loss: 0.0793\n",
            "Epoch 172/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0797 - val_loss: 0.0791\n",
            "Epoch 173/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0797 - val_loss: 0.0793\n",
            "Epoch 174/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0796 - val_loss: 0.0792\n",
            "Epoch 175/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0797 - val_loss: 0.0791\n",
            "Epoch 176/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0796 - val_loss: 0.0791\n",
            "Epoch 177/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0796 - val_loss: 0.0790\n",
            "Epoch 178/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0796 - val_loss: 0.0790\n",
            "Epoch 179/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0796 - val_loss: 0.0792\n",
            "Epoch 180/300\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.0795 - val_loss: 0.0792\n",
            "Epoch 181/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0795 - val_loss: 0.0792\n",
            "Epoch 182/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0795 - val_loss: 0.0790\n",
            "Epoch 183/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0795 - val_loss: 0.0789\n",
            "Epoch 184/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0795 - val_loss: 0.0794\n",
            "Epoch 185/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0795 - val_loss: 0.0845\n",
            "Epoch 186/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0798 - val_loss: 0.0790\n",
            "Epoch 187/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0794 - val_loss: 0.0788\n",
            "Epoch 188/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0794 - val_loss: 0.0789\n",
            "Epoch 189/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0796 - val_loss: 0.0791\n",
            "Epoch 190/300\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.0794 - val_loss: 0.0789\n",
            "Epoch 191/300\n",
            "235/235 [==============================] - 5s 23ms/step - loss: 0.0793 - val_loss: 0.0789\n",
            "Epoch 192/300\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.0792 - val_loss: 0.0788\n",
            "Epoch 193/300\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.0793 - val_loss: 0.0789\n",
            "Epoch 194/300\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.0793 - val_loss: 0.0788\n",
            "Epoch 195/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0792 - val_loss: 0.0788\n",
            "Epoch 196/300\n",
            "235/235 [==============================] - 6s 26ms/step - loss: 0.0792 - val_loss: 0.0788\n",
            "Epoch 197/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0792 - val_loss: 0.0787\n",
            "Epoch 198/300\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.0792 - val_loss: 0.0787\n",
            "Epoch 199/300\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.0792 - val_loss: 0.0789\n",
            "Epoch 200/300\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.0792 - val_loss: 0.0788\n",
            "Epoch 201/300\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.0791 - val_loss: 0.0787\n",
            "Epoch 202/300\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.0791 - val_loss: 0.0787\n",
            "Epoch 203/300\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.0791 - val_loss: 0.0786\n",
            "Epoch 204/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0853 - val_loss: 0.0798\n",
            "Epoch 205/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0799 - val_loss: 0.0791\n",
            "Epoch 206/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0795 - val_loss: 0.0790\n",
            "Epoch 207/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0793 - val_loss: 0.0788\n",
            "Epoch 208/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0792 - val_loss: 0.0788\n",
            "Epoch 209/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0791 - val_loss: 0.0788\n",
            "Epoch 210/300\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.0791 - val_loss: 0.0786\n",
            "Epoch 211/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0791 - val_loss: 0.0788\n",
            "Epoch 212/300\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.0791 - val_loss: 0.0788\n",
            "Epoch 213/300\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.0791 - val_loss: 0.0789\n",
            "Epoch 214/300\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.0803 - val_loss: 0.0789\n",
            "Epoch 215/300\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.0792 - val_loss: 0.0787\n",
            "Epoch 216/300\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.0790 - val_loss: 0.0787\n",
            "Epoch 217/300\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.0790 - val_loss: 0.0789\n",
            "Epoch 218/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0790 - val_loss: 0.0786\n",
            "Epoch 219/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0790 - val_loss: 0.0786\n",
            "Epoch 220/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0790 - val_loss: 0.0788\n",
            "Epoch 221/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0790 - val_loss: 0.0786\n",
            "Epoch 222/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0790 - val_loss: 0.0786\n",
            "Epoch 223/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0790 - val_loss: 0.0789\n",
            "Epoch 224/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0790 - val_loss: 0.0787\n",
            "Epoch 225/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0796 - val_loss: 0.0786\n",
            "Epoch 226/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0790 - val_loss: 0.0785\n",
            "Epoch 227/300\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.0792 - val_loss: 0.0790\n",
            "Epoch 228/300\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.0789 - val_loss: 0.0786\n",
            "Epoch 229/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0789 - val_loss: 0.0785\n",
            "Epoch 230/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0789 - val_loss: 0.0786\n",
            "Epoch 231/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0788 - val_loss: 0.0785\n",
            "Epoch 232/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0788 - val_loss: 0.0785\n",
            "Epoch 233/300\n",
            "235/235 [==============================] - 6s 26ms/step - loss: 0.0788 - val_loss: 0.0787\n",
            "Epoch 234/300\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.0788 - val_loss: 0.0785\n",
            "Epoch 235/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0788 - val_loss: 0.0785\n",
            "Epoch 236/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0788 - val_loss: 0.0785\n",
            "Epoch 237/300\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.0788 - val_loss: 0.0784\n",
            "Epoch 238/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0789 - val_loss: 0.0784\n",
            "Epoch 239/300\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.0788 - val_loss: 0.0785\n",
            "Epoch 240/300\n",
            "235/235 [==============================] - 5s 23ms/step - loss: 0.0787 - val_loss: 0.0785\n",
            "Epoch 241/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0787 - val_loss: 0.0783\n",
            "Epoch 242/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0787 - val_loss: 0.0783\n",
            "Epoch 243/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0789 - val_loss: 0.0784\n",
            "Epoch 244/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0787 - val_loss: 0.0783\n",
            "Epoch 245/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0787 - val_loss: 0.0784\n",
            "Epoch 246/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0788 - val_loss: 0.0788\n",
            "Epoch 247/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0787 - val_loss: 0.0785\n",
            "Epoch 248/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0787 - val_loss: 0.0784\n",
            "Epoch 249/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0787 - val_loss: 0.0784\n",
            "Epoch 250/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0787 - val_loss: 0.0785\n",
            "Epoch 251/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0787 - val_loss: 0.0783\n",
            "Epoch 252/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0786 - val_loss: 0.0785\n",
            "Epoch 253/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0787 - val_loss: 0.0782\n",
            "Epoch 254/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0786 - val_loss: 0.0783\n",
            "Epoch 255/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0792 - val_loss: 0.1282\n",
            "Epoch 256/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0870 - val_loss: 0.0805\n",
            "Epoch 257/300\n",
            "235/235 [==============================] - 5s 23ms/step - loss: 0.0805 - val_loss: 0.0796\n",
            "Epoch 258/300\n",
            "235/235 [==============================] - 5s 23ms/step - loss: 0.0798 - val_loss: 0.0792\n",
            "Epoch 259/300\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.0795 - val_loss: 0.0790\n",
            "Epoch 260/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0793 - val_loss: 0.0788\n",
            "Epoch 261/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0793 - val_loss: 0.0788\n",
            "Epoch 262/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0791 - val_loss: 0.0787\n",
            "Epoch 263/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0790 - val_loss: 0.0786\n",
            "Epoch 264/300\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.0790 - val_loss: 0.0785\n",
            "Epoch 265/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0789 - val_loss: 0.0785\n",
            "Epoch 266/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0789 - val_loss: 0.0785\n",
            "Epoch 267/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0789 - val_loss: 0.0785\n",
            "Epoch 268/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0788 - val_loss: 0.0784\n",
            "Epoch 269/300\n",
            "235/235 [==============================] - 6s 24ms/step - loss: 0.0788 - val_loss: 0.0784\n",
            "Epoch 270/300\n",
            "235/235 [==============================] - 5s 23ms/step - loss: 0.0788 - val_loss: 0.0785\n",
            "Epoch 271/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0788 - val_loss: 0.0788\n",
            "Epoch 272/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0803 - val_loss: 0.0808\n",
            "Epoch 273/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0792 - val_loss: 0.0783\n",
            "Epoch 274/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0787 - val_loss: 0.0784\n",
            "Epoch 275/300\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.0787 - val_loss: 0.0783\n",
            "Epoch 276/300\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.0787 - val_loss: 0.0782\n",
            "Epoch 277/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0786 - val_loss: 0.0783\n",
            "Epoch 278/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0786 - val_loss: 0.0783\n",
            "Epoch 279/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0786 - val_loss: 0.0784\n",
            "Epoch 280/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0814 - val_loss: 0.0794\n",
            "Epoch 281/300\n",
            "235/235 [==============================] - 5s 23ms/step - loss: 0.0793 - val_loss: 0.0786\n",
            "Epoch 282/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0789 - val_loss: 0.0784\n",
            "Epoch 283/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0787 - val_loss: 0.0785\n",
            "Epoch 284/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0786 - val_loss: 0.0782\n",
            "Epoch 285/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0786 - val_loss: 0.0781\n",
            "Epoch 286/300\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.0786 - val_loss: 0.0782\n",
            "Epoch 287/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0786 - val_loss: 0.0782\n",
            "Epoch 288/300\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.0785 - val_loss: 0.0784\n",
            "Epoch 289/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0786 - val_loss: 0.0783\n",
            "Epoch 290/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0785 - val_loss: 0.0781\n",
            "Epoch 291/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0785 - val_loss: 0.0782\n",
            "Epoch 292/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0785 - val_loss: 0.0781\n",
            "Epoch 293/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0785 - val_loss: 0.0781\n",
            "Epoch 294/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0785 - val_loss: 0.0781\n",
            "Epoch 295/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0785 - val_loss: 0.0781\n",
            "Epoch 296/300\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.0785 - val_loss: 0.0782\n",
            "Epoch 297/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0788 - val_loss: 0.0828\n",
            "Epoch 298/300\n",
            "235/235 [==============================] - 6s 26ms/step - loss: 0.0788 - val_loss: 0.0782\n",
            "Epoch 299/300\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.0785 - val_loss: 0.0780\n",
            "Epoch 300/300\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0784 - val_loss: 0.0781\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f6568bcb190>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "predicted_imgs = autoencoder.predict(x_test)\n",
        "# decoded_imgs = autoencoder.predict(encoded_imgs)"
      ],
      "metadata": {
        "id": "tnogr4ppuxl2"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use Matplotlib (don't ask)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "n = 10  # How many digits we will display\n",
        "plt.figure(figsize=(20, 4))\n",
        "for i in range(n):\n",
        "    # Display original\n",
        "    ax = plt.subplot(2, n, i + 1)\n",
        "    plt.imshow(x_test[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    # Display reconstruction\n",
        "    ax = plt.subplot(2, n, i + 1 + n)\n",
        "    plt.imshow(predicted_imgs[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "UXO0OBW3CNYF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "outputId": "5bdbc7d2-a1fe-4949-f478-b892d5084e78"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1440x288 with 20 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OczYkz9SoVJ2"
      },
      "execution_count": 30,
      "outputs": []
    }
  ]
}