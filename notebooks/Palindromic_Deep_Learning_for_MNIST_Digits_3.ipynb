{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Palindromic Deep Learning for MNIST Digits 3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Start with a simple multi-stage autoencoder.\n",
        "\n",
        "Use pRELU since it should be invertible\n",
        "\n",
        "https://blog.keras.io/building-autoencoders-in-keras.html"
      ],
      "metadata": {
        "id": "cKDtwP6aK3AX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "O6-EZfoSkDis"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.ops import nn\n",
        "import keras\n",
        "from keras import layers\n",
        "from keras.datasets import mnist\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "(x_train, _), (x_test, _) = mnist.load_data()\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
        "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCuLZ05oklTv",
        "outputId": "597bd94b-9d5b-4cdc-82a4-14e384799710"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 784)\n",
            "(10000, 784)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model():\n",
        "    input_img = keras.Input(shape=(784,))\n",
        "    encoded = layers.Dense(128)(input_img)\n",
        "    encoded = layers.PReLU()(encoded)\n",
        "    encoded = layers.Dense(64)(encoded)\n",
        "    encoded = layers.PReLU()(encoded)\n",
        "    encoded = layers.Dense(32)(encoded)\n",
        "    encoded = layers.PReLU()(encoded)\n",
        "\n",
        "    decoded = layers.Dense(64)(encoded)\n",
        "    encoded = layers.PReLU()(encoded)\n",
        "    decoded = layers.Dense(128)(decoded)\n",
        "    encoded = layers.PReLU()(encoded)\n",
        "    decoded = layers.Dense(784, activation='sigmoid')(decoded)\n",
        "\n",
        "    autoencoder = keras.Model(input_img, decoded)\n",
        "    autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "    autoencoder.summary()\n",
        "    return autoencoder\n"
      ],
      "metadata": {
        "id": "_7EusGw9l35D"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_model()\n",
        "model.fit(x_train, x_train,\n",
        "                epochs=5,\n",
        "                batch_size=256,\n",
        "                shuffle=True)\n",
        "prelu = []\n",
        "for layer in model.layers:\n",
        "    if 'p_re_lu' in layer.name:\n",
        "        prelu.append(layer)\n",
        "print(prelu)"
      ],
      "metadata": {
        "id": "T8pVaeGCp1cj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69144341-f620-4de8-9029-1bec36eb439a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 784)]             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               100480    \n",
            "                                                                 \n",
            " p_re_lu (PReLU)             (None, 128)               128       \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " p_re_lu_1 (PReLU)           (None, 64)                64        \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 32)                2080      \n",
            "                                                                 \n",
            " p_re_lu_2 (PReLU)           (None, 32)                32        \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 64)                2112      \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 128)               8320      \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 784)               101136    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 222,608\n",
            "Trainable params: 222,608\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "235/235 [==============================] - 9s 31ms/step - loss: 0.2305\n",
            "Epoch 2/5\n",
            "235/235 [==============================] - 8s 32ms/step - loss: 0.1438\n",
            "Epoch 3/5\n",
            "235/235 [==============================] - 8s 32ms/step - loss: 0.1261\n",
            "Epoch 4/5\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.1180\n",
            "Epoch 5/5\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 0.1130\n",
            "[<keras.layers.advanced_activations.PReLU object at 0x7f656fc27790>, <keras.layers.advanced_activations.PReLU object at 0x7f656bee9c10>, <keras.layers.advanced_activations.PReLU object at 0x7f656bef5950>]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for p in prelu:\n",
        "    print(p.trainable_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUSQ5d7mwlOW",
        "outputId": "8170a636-697d-4426-c57a-0634a9454254"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[<tf.Variable 'p_re_lu/alpha:0' shape=(128,) dtype=float32, numpy=\n",
            "array([ 9.71000548e-03,  7.32508302e-02,  2.85959523e-02, -3.82519588e-02,\n",
            "        2.90668815e-01,  1.69363156e-01,  2.45097190e-01,  6.92224801e-02,\n",
            "       -5.32902889e-02,  9.22408514e-03,  1.10214159e-01,  4.06620741e-01,\n",
            "        1.24493331e-01,  3.46499383e-02,  3.02098729e-02,  4.21229675e-02,\n",
            "        2.98868477e-01,  2.22231708e-02,  1.12229533e-01,  2.89825916e-01,\n",
            "        3.25130165e-01, -5.59885390e-02,  1.35369763e-01,  1.30249247e-01,\n",
            "       -1.26103759e-02,  3.42662275e-01, -2.90157795e-02, -9.65612009e-02,\n",
            "        1.44173875e-01,  4.54036929e-02,  1.65743247e-01,  6.32910952e-02,\n",
            "        2.11370625e-02,  2.11678296e-01,  1.25232348e-02,  1.32398292e-01,\n",
            "        6.15807056e-01,  1.80929542e-01,  2.34870955e-01, -4.58888300e-02,\n",
            "        1.54503629e-01, -8.20528995e-03,  1.62208937e-02, -6.45974725e-02,\n",
            "        6.33314773e-02,  2.06263199e-01, -2.18371954e-02,  2.42061242e-01,\n",
            "        2.03945354e-01, -1.22664450e-02,  2.66596287e-01,  9.93652418e-02,\n",
            "        9.88856107e-02,  1.53210089e-01, -5.71680777e-02,  4.94368747e-02,\n",
            "        2.05393299e-01,  1.78902131e-02,  3.17077190e-01,  2.55800843e-01,\n",
            "       -1.52864475e-02,  2.20335335e-01,  3.14946733e-02,  1.34599274e-02,\n",
            "       -7.78835267e-02,  1.54341787e-01,  3.80928516e-01,  2.45806485e-01,\n",
            "        2.84465075e-01,  1.50934188e-02, -1.43878348e-02,  2.34600514e-01,\n",
            "        7.51729235e-02,  2.13643372e-01,  7.11393505e-02, -1.23815127e-01,\n",
            "       -4.32926882e-03,  2.75603607e-02,  1.49285048e-01,  2.14994848e-01,\n",
            "        1.39039949e-01,  7.01290295e-02, -5.74928932e-02,  4.46177535e-02,\n",
            "        2.41342321e-01,  4.62799929e-02,  4.89503406e-02,  1.66762754e-01,\n",
            "        2.41732284e-01, -4.54005972e-03,  1.01997271e-01,  4.13742103e-02,\n",
            "       -6.80320710e-02,  8.82252492e-03,  5.08445017e-02,  1.87278334e-02,\n",
            "       -7.42452666e-02,  9.60477963e-02,  5.16416971e-03, -9.16333124e-03,\n",
            "        1.33603856e-01,  2.10742950e-01,  4.38435040e-02,  8.21922161e-03,\n",
            "       -9.87442472e-05,  1.93024144e-01, -8.07401836e-02,  1.87361822e-01,\n",
            "        2.00332746e-01, -2.24031415e-02,  1.56527355e-01, -4.23848443e-02,\n",
            "        1.59523249e-01,  6.84592575e-02,  4.38913293e-02,  3.76401981e-03,\n",
            "       -5.46306819e-02,  8.33855271e-02,  5.81071414e-02,  5.61144501e-02,\n",
            "        4.39076692e-01, -6.57723993e-02,  3.26611429e-01,  1.84144899e-01,\n",
            "       -1.78160235e-01,  1.77290216e-02,  3.52010422e-04,  1.81584075e-01],\n",
            "      dtype=float32)>]\n",
            "[<tf.Variable 'p_re_lu_1/alpha:0' shape=(64,) dtype=float32, numpy=\n",
            "array([-0.08459727,  0.02970668,  0.18978347,  0.24316248,  0.02958869,\n",
            "        0.27815098, -0.16556853,  0.10771832,  0.08422387,  0.06842755,\n",
            "        0.00283065,  0.07932265, -0.03027955,  0.45853427,  0.04370722,\n",
            "        0.03896458,  0.06004128,  0.19605875,  0.17862965,  0.13282186,\n",
            "        0.08710887,  0.12032916,  0.07790282,  0.31616917,  0.16128041,\n",
            "        0.09134354,  0.16374831, -0.05199543,  0.03486546,  0.02624284,\n",
            "        0.07970461,  0.14253949,  0.07366389,  0.07723868,  0.05209094,\n",
            "        0.24717496,  0.05653078,  0.2651357 ,  0.03388922,  0.03959306,\n",
            "       -0.01941862,  0.09318934,  0.03697772,  0.05655083,  0.22435327,\n",
            "        0.09984128,  0.10614087,  0.10313675,  0.72122836,  0.08839626,\n",
            "        0.03612828,  0.11355543,  0.07348163,  0.0060852 ,  0.5491415 ,\n",
            "        0.26610938, -0.00114821,  0.17477882,  0.24340339,  0.16727704,\n",
            "        0.19019268,  0.6014464 ,  0.11236307,  0.02997275], dtype=float32)>]\n",
            "[<tf.Variable 'p_re_lu_2/alpha:0' shape=(32,) dtype=float32, numpy=\n",
            "array([ 0.0839026 ,  0.10446625,  0.4065582 ,  0.3796923 ,  0.32916716,\n",
            "       -0.04306151,  0.5577704 ,  0.06742236, -0.03867987,  0.39177316,\n",
            "        0.66426235, -0.00613852,  0.32710952,  0.3913605 , -0.02634487,\n",
            "        0.2022771 ,  0.31886134,  0.01764531,  0.        ,  0.25294468,\n",
            "        0.17672023,  0.30525354,  0.33727393, -0.04026616,  0.00372353,\n",
            "        0.30101228,  0.3656675 ,  0.62044257,  0.27083576,  0.1976221 ,\n",
            "        0.02071282,  0.11437317], dtype=float32)>]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TiedDense(layers.Layer):\n",
        "    def __init__(self, master_layer):\n",
        "        super(TiedDense, self).__init__()\n",
        "        self.master_layer = master_layer\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # do not train weights or bias from master_layer, they are read-only\n",
        "        self.params = []\n",
        "        \n",
        "    def call(self, inputs):  # Defines the computation from inputs to outputs\n",
        "        W = self.master_layer._trainable_weights[0]\n",
        "        b = self.master_layer._trainable_weights[1]\n",
        "        w = tf.transpose(W)\n",
        "        return tf.matmul(inputs - b, w)\n",
        "    "
      ],
      "metadata": {
        "id": "C9LallIk5HHD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class TiedPReLU(layers.Layer):\n",
        "    def __init__(self, master_layer):\n",
        "        super(TiedPReLU, self).__init__()\n",
        "        self.master_layer = master_layer\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # do not train weights or bias from master_layer, they are read-only\n",
        "        self.params = []\n",
        "\n",
        "    def call(self, inputs):\n",
        "        alpha = 1/(self.master_layer.alpha + 0.00001)\n",
        "        pos = keras.backend.relu(inputs)\n",
        "        neg = -alpha * keras.backend.relu(-inputs)\n",
        "        return pos + neg\n"
      ],
      "metadata": {
        "id": "IZ8KUvUqy2rj"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kernel_init='he_normal'\n",
        "paper_init=keras.initializers.Constant(0.25)\n",
        "def create_palindromic_model():\n",
        "    input_img = keras.Input(shape=(784,))\n",
        "    a = layers.Dense(128, kernel_initializer=kernel_init)\n",
        "    encoded = a(input_img)\n",
        "    a_p = layers.PReLU(alpha_initializer=paper_init)\n",
        "    encoded = a_p(encoded)\n",
        "    # encoded = layers.Dropout(0.2)(encoded)\n",
        "    b = layers.Dense(64, kernel_initializer=kernel_init)\n",
        "    encoded = b(encoded)\n",
        "    b_p = layers.PReLU(alpha_initializer=paper_init)\n",
        "    encoded = b_p(encoded)\n",
        "    c = layers.Dense(32, kernel_initializer=kernel_init)\n",
        "    encoded = c(encoded)\n",
        "    c_p = layers.PReLU(alpha_initializer=paper_init)\n",
        "    encoded = c_p(encoded)\n",
        "\n",
        "    embedding = encoded\n",
        "\n",
        "    decoded = TiedPReLU(c_p)(embedding)\n",
        "    decoded = TiedDense(c)(decoded)\n",
        "    decoded = TiedPReLU(b_p)(decoded)\n",
        "    decoded = TiedDense(b)(decoded)\n",
        "    decoded = TiedPReLU(a_p)(decoded)\n",
        "    decoded = TiedDense(a)(decoded)\n",
        "    decoded = layers.Activation(activation='sigmoid')(decoded)\n",
        "\n",
        "    autoencoder = keras.Model(input_img, decoded)\n",
        "    autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "    autoencoder.summary()\n",
        "    return autoencoder\n"
      ],
      "metadata": {
        "id": "ZVpLJHQEpDwO"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 100\n",
        "autoencoder = create_palindromic_model()\n",
        "\n",
        "autoencoder.fit(x_train, x_train,\n",
        "                epochs=num_epochs,\n",
        "                batch_size=256,\n",
        "                shuffle=True,\n",
        "                validation_data=(x_test, x_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8AHMi-ttlDJ",
        "outputId": "6c43fde0-2b4d-42ac-bd1e-b1c719a53af8"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_5 (InputLayer)        [(None, 784)]             0         \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 128)               100480    \n",
            "                                                                 \n",
            " p_re_lu_14 (PReLU)          (None, 128)               128       \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 64)                8256      \n",
            "                                                                 \n",
            " p_re_lu_15 (PReLU)          (None, 64)                64        \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 32)                2080      \n",
            "                                                                 \n",
            " p_re_lu_16 (PReLU)          (None, 32)                32        \n",
            "                                                                 \n",
            " tied_p_re_lu_9 (TiedPReLU)  (None, 32)                32        \n",
            "                                                                 \n",
            " tied_dense_9 (TiedDense)    (None, 64)                2080      \n",
            "                                                                 \n",
            " tied_p_re_lu_10 (TiedPReLU)  (None, 64)               64        \n",
            "                                                                 \n",
            " tied_dense_10 (TiedDense)   (None, 128)               8256      \n",
            "                                                                 \n",
            " tied_p_re_lu_11 (TiedPReLU)  (None, 128)              128       \n",
            "                                                                 \n",
            " tied_dense_11 (TiedDense)   (None, 784)               100480    \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 784)               0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 111,040\n",
            "Trainable params: 111,040\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "235/235 [==============================] - 6s 21ms/step - loss: 0.2598 - val_loss: 0.1612\n",
            "Epoch 2/100\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.1389 - val_loss: 0.1216\n",
            "Epoch 3/100\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.1166 - val_loss: 0.1096\n",
            "Epoch 4/100\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.1075 - val_loss: 0.1034\n",
            "Epoch 5/100\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.1028 - val_loss: 0.0997\n",
            "Epoch 6/100\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.1000 - val_loss: 0.0977\n",
            "Epoch 7/100\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0980 - val_loss: 0.0962\n",
            "Epoch 8/100\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0964 - val_loss: 0.0942\n",
            "Epoch 9/100\n",
            "235/235 [==============================] - 6s 25ms/step - loss: 0.0951 - val_loss: 0.0932\n",
            "Epoch 10/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0942 - val_loss: 0.0925\n",
            "Epoch 11/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0933 - val_loss: 0.0917\n",
            "Epoch 12/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0927 - val_loss: 0.0912\n",
            "Epoch 13/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0922 - val_loss: 0.0909\n",
            "Epoch 14/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0918 - val_loss: 0.0903\n",
            "Epoch 15/100\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0914 - val_loss: 0.0901\n",
            "Epoch 16/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0910 - val_loss: 0.0898\n",
            "Epoch 17/100\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0907 - val_loss: 0.0893\n",
            "Epoch 18/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0903 - val_loss: 0.0893\n",
            "Epoch 19/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0900 - val_loss: 0.0887\n",
            "Epoch 20/100\n",
            "235/235 [==============================] - 5s 23ms/step - loss: 0.0896 - val_loss: 0.0883\n",
            "Epoch 21/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0894 - val_loss: 0.0887\n",
            "Epoch 22/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0890 - val_loss: 0.0877\n",
            "Epoch 23/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0894 - val_loss: 0.0876\n",
            "Epoch 24/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0884 - val_loss: 0.0871\n",
            "Epoch 25/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0881 - val_loss: 0.0871\n",
            "Epoch 26/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0879 - val_loss: 0.0866\n",
            "Epoch 27/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0875 - val_loss: 0.0864\n",
            "Epoch 28/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0876 - val_loss: 0.0909\n",
            "Epoch 29/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0873 - val_loss: 0.0861\n",
            "Epoch 30/100\n",
            "235/235 [==============================] - 6s 26ms/step - loss: 0.0869 - val_loss: 0.0857\n",
            "Epoch 31/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0867 - val_loss: 0.0857\n",
            "Epoch 32/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0865 - val_loss: 0.0858\n",
            "Epoch 33/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0862 - val_loss: 0.0854\n",
            "Epoch 34/100\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.0860 - val_loss: 0.0851\n",
            "Epoch 35/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0859 - val_loss: 0.0849\n",
            "Epoch 36/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0953 - val_loss: 0.0872\n",
            "Epoch 37/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0875 - val_loss: 0.0860\n",
            "Epoch 38/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0867 - val_loss: 0.0856\n",
            "Epoch 39/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0863 - val_loss: 0.0852\n",
            "Epoch 40/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0860 - val_loss: 0.0852\n",
            "Epoch 41/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0858 - val_loss: 0.0847\n",
            "Epoch 42/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0858 - val_loss: 0.0847\n",
            "Epoch 43/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0855 - val_loss: 0.0846\n",
            "Epoch 44/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0853 - val_loss: 0.0843\n",
            "Epoch 45/100\n",
            "235/235 [==============================] - 6s 26ms/step - loss: 0.0852 - val_loss: 0.0844\n",
            "Epoch 46/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0851 - val_loss: 0.0850\n",
            "Epoch 47/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0850 - val_loss: 0.0858\n",
            "Epoch 48/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0849 - val_loss: 0.0844\n",
            "Epoch 49/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0847 - val_loss: 0.0838\n",
            "Epoch 50/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0845 - val_loss: 0.0840\n",
            "Epoch 51/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0845 - val_loss: 0.0848\n",
            "Epoch 52/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0929 - val_loss: 0.0866\n",
            "Epoch 53/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0865 - val_loss: 0.0849\n",
            "Epoch 54/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0853 - val_loss: 0.0841\n",
            "Epoch 55/100\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0847 - val_loss: 0.0836\n",
            "Epoch 56/100\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0844 - val_loss: 0.0836\n",
            "Epoch 57/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0842 - val_loss: 0.0835\n",
            "Epoch 58/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0840 - val_loss: 0.0831\n",
            "Epoch 59/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0843 - val_loss: 0.0862\n",
            "Epoch 60/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0842 - val_loss: 0.0831\n",
            "Epoch 61/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0838 - val_loss: 0.0837\n",
            "Epoch 62/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0837 - val_loss: 0.0831\n",
            "Epoch 63/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0836 - val_loss: 0.0829\n",
            "Epoch 64/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0835 - val_loss: 0.0828\n",
            "Epoch 65/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0836 - val_loss: 0.0829\n",
            "Epoch 66/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0834 - val_loss: 0.0826\n",
            "Epoch 67/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0833 - val_loss: 0.0826\n",
            "Epoch 68/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0833 - val_loss: 0.0825\n",
            "Epoch 69/100\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.0832 - val_loss: 0.0826\n",
            "Epoch 70/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0831 - val_loss: 0.0823\n",
            "Epoch 71/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0830 - val_loss: 0.0822\n",
            "Epoch 72/100\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0830 - val_loss: 0.0828\n",
            "Epoch 73/100\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0841 - val_loss: 0.0829\n",
            "Epoch 74/100\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0830 - val_loss: 0.0819\n",
            "Epoch 75/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0827 - val_loss: 0.0820\n",
            "Epoch 76/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0827 - val_loss: 0.0821\n",
            "Epoch 77/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0826 - val_loss: 0.0821\n",
            "Epoch 78/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0828 - val_loss: 0.0823\n",
            "Epoch 79/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0826 - val_loss: 0.0818\n",
            "Epoch 80/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0824 - val_loss: 0.0822\n",
            "Epoch 81/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0824 - val_loss: 0.0823\n",
            "Epoch 82/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0824 - val_loss: 0.0818\n",
            "Epoch 83/100\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.0823 - val_loss: 0.0816\n",
            "Epoch 84/100\n",
            "235/235 [==============================] - 5s 23ms/step - loss: 0.0822 - val_loss: 0.0816\n",
            "Epoch 85/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0821 - val_loss: 0.0817\n",
            "Epoch 86/100\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0826 - val_loss: 0.0815\n",
            "Epoch 87/100\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0823 - val_loss: 0.0815\n",
            "Epoch 88/100\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0820 - val_loss: 0.0817\n",
            "Epoch 89/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0819 - val_loss: 0.0814\n",
            "Epoch 90/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0818 - val_loss: 0.0814\n",
            "Epoch 91/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0818 - val_loss: 0.0813\n",
            "Epoch 92/100\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0820 - val_loss: 0.0814\n",
            "Epoch 93/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0818 - val_loss: 0.0819\n",
            "Epoch 94/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0817 - val_loss: 0.0811\n",
            "Epoch 95/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0817 - val_loss: 0.0812\n",
            "Epoch 96/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0817 - val_loss: 0.0813\n",
            "Epoch 97/100\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0815 - val_loss: 0.0808\n",
            "Epoch 98/100\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0815 - val_loss: 0.0818\n",
            "Epoch 99/100\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0816 - val_loss: 0.0813\n",
            "Epoch 100/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0814 - val_loss: 0.0809\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f655e9b4490>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "predicted_imgs = autoencoder.predict(x_test)\n",
        "# decoded_imgs = autoencoder.predict(encoded_imgs)"
      ],
      "metadata": {
        "id": "tnogr4ppuxl2"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use Matplotlib (don't ask)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "n = 10  # How many digits we will display\n",
        "plt.figure(figsize=(20, 4))\n",
        "for i in range(n):\n",
        "    # Display original\n",
        "    ax = plt.subplot(2, n, i + 1)\n",
        "    plt.imshow(x_test[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    # Display reconstruction\n",
        "    ax = plt.subplot(2, n, i + 1 + n)\n",
        "    plt.imshow(predicted_imgs[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "UXO0OBW3CNYF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "outputId": "5cca6ebc-f453-4699-b742-2dc9228f33b6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1440x288 with 20 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OczYkz9SoVJ2"
      },
      "execution_count": 21,
      "outputs": []
    }
  ]
}